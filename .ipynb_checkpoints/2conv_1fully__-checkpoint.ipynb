{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n"
     ]
    }
   ],
   "source": [
    "#conv Neural Network\n",
    "# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "\"\"\"\n",
    "created by Shim Kyu Seok\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "file_locate='/home/user01/notebook/Mnist_Data/'\n",
    "\n",
    "\n",
    "img_row = 28\n",
    "img_col = 28\n",
    "in_ch =1\n",
    "\n",
    "\n",
    "learning_rate=0.0001\n",
    "batch_size=64\n",
    "n_classes =10\n",
    "\n",
    "\n",
    "weight_row =3 ; weight_col=3\n",
    "out_ch1=100\n",
    "out_ch2=100\n",
    "out_ch3=200\n",
    "out_ch4=400\n",
    "fully_ch1=1024\n",
    "\n",
    "strides_1=[1,1,1,1]\n",
    "strides_2=[1,1,1,1]\n",
    "strides_3=[1,1,1,1]\n",
    "strides_4=[1,1,1,1]\n",
    "iterate=50000\n",
    "\n",
    "\n",
    "device_ = '/gpu:0'\n",
    "\n",
    "\n",
    "\n",
    "with tf.device(device_):\n",
    "    x= tf.placeholder(\"float\",shape=[batch_size,img_col * img_row * in_ch],  name = 'x-input')\n",
    "    y_=tf.placeholder(\"float\",shape=[batch_size, n_classes] , name = 'y-input')\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    x_image= tf.reshape(x,[-1,img_row,img_col,in_ch])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print img_col , img_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (55000, 784)\n",
      "Training Data Label (55000, 10)\n",
      "Test Data Label (10000, 10)\n",
      "val Data Label (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    #with tf.device('/gpu:2'):\n",
    "    train_img=np.load(file_locate+'train_img.npy');\n",
    "    train_lab=np.load(file_locate+'train_lab.npy');\n",
    "    val_img= np.load(file_locate+'val_img.npy');\n",
    "    val_lab = np.load(file_locate+'val_lab.npy');\n",
    "    test_img=np.load(file_locate+'test_img.npy');\n",
    "    test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "    print \"Training Data\",np.shape(train_img)\n",
    "    print \"Training Data Label\",np.shape(train_lab)\n",
    "    print \"Test Data Label\",np.shape(test_lab)\n",
    "    print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "    n_train= np.shape(train_img)[0]\n",
    "    n_train_lab = np.shape(train_lab)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def weight_variable(name,shape):\n",
    "    #initial = tf.truncated_normal(shape , stddev=0.1)\n",
    "    initial = tf.get_variable(name,shape=shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return tf.Variable(initial)\"\"\"\n",
    "with tf.device('/gpu:2'):\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1 , shape=shape)\n",
    "        return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    def conv2d(x,w,strides_):\n",
    "        return tf.nn.conv2d(x,w, strides = strides_, padding='SAME')\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x , ksize=[1,2,2,1] ,strides = [1,2,2,1] , padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    with tf.variable_scope(\"layer1\") as scope:\n",
    "        try:\n",
    "            w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    with tf.variable_scope(\"layer1\") as scope:\n",
    "        try:\n",
    "            b_conv1 = bias_variable([out_ch1])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv1 = bias_variable([out_ch1])\n",
    "    with tf.variable_scope('layer2') as scope:\n",
    "        try:\n",
    "            w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.variable_scope('layer2') as scope:\n",
    "        try:\n",
    "            b_conv2= bias_variable([out_ch2])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv2= bias_variable([out_ch2])\n",
    "            \n",
    "    with tf.variable_scope('layer3') as scope:\n",
    "        try:\n",
    "            w_conv3 = tf.get_variable(\"W3\",[weight_row,weight_col,out_ch2,out_ch3])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv3 = tf.get_variable(\"W3\",[weight_row,weight_col,out_ch2,out_ch3])\n",
    "\n",
    "    with tf.variable_scope('layer3') as scope:\n",
    "        try:\n",
    "            b_conv3= bias_variable([out_ch3])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv3= bias_variable([out_ch3])            \n",
    "\n",
    "    with tf.variable_scope('layer4') as scope:\n",
    "        try:\n",
    "            w_conv4 = tf.get_variable(\"W4\",[weight_row,weight_col,out_ch3,out_ch4])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv4 = tf.get_variable(\"W4\",[weight_row,weight_col,out_ch3,out_ch4])\n",
    "\n",
    "    with tf.variable_scope('layer4') as scope:\n",
    "        try:\n",
    "            b_conv4= bias_variable([out_ch4])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv4= bias_variable([out_ch4])                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_3:0\", shape=(64, 14, 14, 100), dtype=float32, device=/device:GPU:2)\n",
      "Tensor(\"MaxPool_4:0\", shape=(64, 7, 7, 100), dtype=float32, device=/device:GPU:2)\n",
      "Tensor(\"MaxPool_5:0\", shape=(64, 4, 4, 200), dtype=float32, device=/device:GPU:2)\n",
      "Tensor(\"MaxPool_6:0\", shape=(64, 2, 2, 400), dtype=float32, device=/device:GPU:2)\n"
     ]
    }
   ],
   "source": [
    "#conncect hidden layer \n",
    "with tf.device('/gpu:2'):\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image , w_conv1 ,strides_1)+b_conv1)\n",
    "    h_conv1 = max_pool_2x2(h_conv1)#pooling\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1 , w_conv2 ,strides_2)+b_conv2)\n",
    "    h_conv2 = max_pool_2x2(h_conv2)#pooling\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2 , w_conv3 ,strides_3)+b_conv3)\n",
    "    h_conv3 = max_pool_2x2(h_conv3)#pooling\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3 , w_conv4 ,strides_4)+b_conv4)\n",
    "    h_conv4 = max_pool_2x2(h_conv4)#pooling\n",
    "   \n",
    "    print h_conv1\n",
    "    print h_conv2\n",
    "    print h_conv3   \n",
    "    print h_conv4\n",
    "  \n",
    "\n",
    "    #print conv2d(h_pool1 , w_conv2).get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "\n",
    "    end_conv = h_conv2\n",
    "    end_conv_row=int(end_conv.get_shape()[1])\n",
    "    end_conv_col=int(end_conv.get_shape()[2])\n",
    "    end_conv_ch=int(end_conv.get_shape()[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connect fully connected layer \n",
    "with tf.device('/gpu:2'):\n",
    "    with tf.variable_scope(\"fc1\") as scope:\n",
    "        try:\n",
    "            w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc1 = bias_variable([fully_ch1])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc1 = bias_variable([fully_ch1])\n",
    "\n",
    "        \n",
    "with tf.device('/gpu:2'): # flat conv layer \n",
    "    end_flat_conv =tf.reshape(end_conv, [-1,end_conv_col*end_conv_row*end_conv_ch])\n",
    "   \n",
    "with tf.device('/gpu:2'): # connect flat layer with fully  connnected layer \n",
    "    h_fc1 = tf.nn.relu(tf.matmul(end_flat_conv , w_fc1)+ b_fc1)\n",
    "    h_fc1 = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    end_fc=h_fc1\n",
    "    end_ch=end_fc.get_shape()[1]\n",
    "    print end_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "    with tf.variable_scope('fc3') as scope:\n",
    "        try:\n",
    "            w_end =tf.get_variable(\"end_W\",[end_ch , n_classes ],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_end =tf.get_variable(\"end_W\",[end_ch , n_classes],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_end = bias_variable([n_classes])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_end = bias_variable([n_classes])\n",
    "\n",
    "with tf.device('/gpu:2'):  # join flat layer with fully  connnected layer \n",
    "    y_conv = tf.matmul(end_fc , w_end)+b_end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-476f61a8ea42>:18 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "#sm_conv= tf.nn.softmax(y_conv)\n",
    "    #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    start_time = time.time()\n",
    "\n",
    "    #regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_))\n",
    "with tf.device('/gpu:2'):\n",
    "    #cost = cost+regular\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost) #1e-4\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , validation  accuracy 0.203125\n",
      "step 0 , validation loss : 2.30567\n",
      "step 100 , validation  accuracy 0.875\n",
      "step 100 , validation loss : 0.702653\n",
      "step 200 , validation  accuracy 0.921875\n",
      "step 200 , validation loss : 0.46288\n",
      "step 300 , validation  accuracy 0.921875\n",
      "step 300 , validation loss : 0.397724\n",
      "step 400 , validation  accuracy 0.90625\n",
      "step 400 , validation loss : 0.332476\n",
      "step 500 , validation  accuracy 0.9375\n",
      "step 500 , validation loss : 0.303398\n",
      "step 600 , validation  accuracy 0.953125\n",
      "step 600 , validation loss : 0.318196\n",
      "step 700 , validation  accuracy 0.953125\n",
      "step 700 , validation loss : 0.264371\n",
      "step 800 , validation  accuracy 0.953125\n",
      "step 800 , validation loss : 0.256074\n",
      "step 900 , validation  accuracy 0.953125\n",
      "step 900 , validation loss : 0.216631\n",
      "step 1000 , validation  accuracy 0.953125\n",
      "step 1000 , validation loss : 0.242734\n",
      "step 1100 , validation  accuracy 0.9375\n",
      "step 1100 , validation loss : 0.225968\n",
      "step 1200 , validation  accuracy 0.9375\n",
      "step 1200 , validation loss : 0.220744\n",
      "step 1300 , validation  accuracy 0.9375\n",
      "step 1300 , validation loss : 0.237855\n",
      "step 1400 , validation  accuracy 0.953125\n",
      "step 1400 , validation loss : 0.249752\n",
      "step 1500 , validation  accuracy 0.953125\n",
      "step 1500 , validation loss : 0.180176\n",
      "step 1600 , validation  accuracy 0.953125\n",
      "step 1600 , validation loss : 0.130065\n",
      "step 1700 , validation  accuracy 0.953125\n",
      "step 1700 , validation loss : 0.202872\n",
      "step 1800 , validation  accuracy 0.953125\n",
      "step 1800 , validation loss : 0.230824\n",
      "step 1900 , validation  accuracy 0.953125\n",
      "step 1900 , validation loss : 0.178343\n",
      "step 2000 , validation  accuracy 0.953125\n",
      "step 2000 , validation loss : 0.193931\n",
      "step 2100 , validation  accuracy 0.953125\n",
      "step 2100 , validation loss : 0.120271\n",
      "step 2200 , validation  accuracy 0.953125\n",
      "step 2200 , validation loss : 0.158277\n",
      "step 2300 , validation  accuracy 0.953125\n",
      "step 2300 , validation loss : 0.104233\n",
      "step 2400 , validation  accuracy 0.96875\n",
      "step 2400 , validation loss : 0.215844\n",
      "step 2500 , validation  accuracy 0.953125\n",
      "step 2500 , validation loss : 0.183814\n",
      "step 2600 , validation  accuracy 0.96875\n",
      "step 2600 , validation loss : 0.0934699\n",
      "step 2700 , validation  accuracy 0.953125\n",
      "step 2700 , validation loss : 0.123852\n",
      "step 2800 , validation  accuracy 0.984375\n",
      "step 2800 , validation loss : 0.0622958\n",
      "step 2900 , validation  accuracy 0.953125\n",
      "step 2900 , validation loss : 0.0959586\n",
      "step 3000 , validation  accuracy 0.96875\n",
      "step 3000 , validation loss : 0.11076\n",
      "step 3100 , validation  accuracy 0.953125\n",
      "step 3100 , validation loss : 0.134106\n",
      "step 3200 , validation  accuracy 0.953125\n",
      "step 3200 , validation loss : 0.113983\n",
      "step 3300 , validation  accuracy 0.9375\n",
      "step 3300 , validation loss : 0.107157\n",
      "step 3400 , validation  accuracy 0.96875\n",
      "step 3400 , validation loss : 0.105633\n",
      "step 3500 , validation  accuracy 0.96875\n",
      "step 3500 , validation loss : 0.138844\n",
      "step 3600 , validation  accuracy 0.96875\n",
      "step 3600 , validation loss : 0.120512\n",
      "step 3700 , validation  accuracy 0.96875\n",
      "step 3700 , validation loss : 0.132591\n",
      "step 3800 , validation  accuracy 0.96875\n",
      "step 3800 , validation loss : 0.0628463\n",
      "step 3900 , validation  accuracy 0.953125\n",
      "step 3900 , validation loss : 0.125936\n",
      "step 4000 , validation  accuracy 0.96875\n",
      "step 4000 , validation loss : 0.0744973\n",
      "step 4100 , validation  accuracy 0.96875\n",
      "step 4100 , validation loss : 0.106682\n",
      "step 4200 , validation  accuracy 0.96875\n",
      "step 4200 , validation loss : 0.08273\n",
      "step 4300 , validation  accuracy 0.96875\n",
      "step 4300 , validation loss : 0.137946\n",
      "step 4400 , validation  accuracy 0.96875\n",
      "step 4400 , validation loss : 0.102407\n",
      "step 4500 , validation  accuracy 0.984375\n",
      "step 4500 , validation loss : 0.126242\n",
      "step 4600 , validation  accuracy 0.953125\n",
      "step 4600 , validation loss : 0.10507\n",
      "step 4700 , validation  accuracy 0.953125\n",
      "step 4700 , validation loss : 0.112767\n",
      "step 4800 , validation  accuracy 0.96875\n",
      "step 4800 , validation loss : 0.0784229\n",
      "step 4900 , validation  accuracy 0.953125\n",
      "step 4900 , validation loss : 0.147323\n",
      "step 5000 , validation  accuracy 0.96875\n",
      "step 5000 , validation loss : 0.14346\n",
      "step 5100 , validation  accuracy 0.96875\n",
      "step 5100 , validation loss : 0.108299\n",
      "step 5200 , validation  accuracy 0.953125\n",
      "step 5200 , validation loss : 0.0790232\n",
      "step 5300 , validation  accuracy 0.953125\n",
      "step 5300 , validation loss : 0.12845\n",
      "step 5400 , validation  accuracy 0.96875\n",
      "step 5400 , validation loss : 0.103691\n",
      "step 5500 , validation  accuracy 0.96875\n",
      "step 5500 , validation loss : 0.0756644\n",
      "step 5600 , validation  accuracy 0.96875\n",
      "step 5600 , validation loss : 0.106194\n",
      "step 5700 , validation  accuracy 0.984375\n",
      "step 5700 , validation loss : 0.0827717\n",
      "step 5800 , validation  accuracy 0.96875\n",
      "step 5800 , validation loss : 0.111894\n",
      "step 5900 , validation  accuracy 0.984375\n",
      "step 5900 , validation loss : 0.0855272\n",
      "step 6000 , validation  accuracy 0.953125\n",
      "step 6000 , validation loss : 0.119263\n",
      "step 6100 , validation  accuracy 0.984375\n",
      "step 6100 , validation loss : 0.0709449\n",
      "step 6200 , validation  accuracy 0.96875\n",
      "step 6200 , validation loss : 0.0736656\n",
      "step 6300 , validation  accuracy 0.96875\n",
      "step 6300 , validation loss : 0.0587424\n",
      "step 6400 , validation  accuracy 0.96875\n",
      "step 6400 , validation loss : 0.0935741\n",
      "step 6500 , validation  accuracy 0.984375\n",
      "step 6500 , validation loss : 0.0320997\n",
      "step 6600 , validation  accuracy 0.984375\n",
      "step 6600 , validation loss : 0.0717281\n",
      "step 6700 , validation  accuracy 0.984375\n",
      "step 6700 , validation loss : 0.0273533\n",
      "step 6800 , validation  accuracy 0.984375\n",
      "step 6800 , validation loss : 0.0621838\n",
      "step 6900 , validation  accuracy 0.984375\n",
      "step 6900 , validation loss : 0.0568441\n",
      "step 7000 , validation  accuracy 0.984375\n",
      "step 7000 , validation loss : 0.0471331\n",
      "step 7100 , validation  accuracy 0.984375\n",
      "step 7100 , validation loss : 0.0436761\n",
      "step 7200 , validation  accuracy 0.96875\n",
      "step 7200 , validation loss : 0.109147\n",
      "step 7300 , validation  accuracy 0.984375\n",
      "step 7300 , validation loss : 0.0724615\n",
      "step 7400 , validation  accuracy 0.96875\n",
      "step 7400 , validation loss : 0.117425\n",
      "step 7500 , validation  accuracy 1\n",
      "step 7500 , validation loss : 0.0257404\n",
      "step 7600 , validation  accuracy 0.984375\n",
      "step 7600 , validation loss : 0.0761722\n",
      "step 7700 , validation  accuracy 0.96875\n",
      "step 7700 , validation loss : 0.0813425\n",
      "step 7800 , validation  accuracy 0.984375\n",
      "step 7800 , validation loss : 0.0811248\n",
      "step 7900 , validation  accuracy 0.984375\n",
      "step 7900 , validation loss : 0.0514143\n",
      "step 8000 , validation  accuracy 0.984375\n",
      "step 8000 , validation loss : 0.0218754\n",
      "step 8100 , validation  accuracy 0.984375\n",
      "step 8100 , validation loss : 0.0590557\n",
      "step 8200 , validation  accuracy 0.953125\n",
      "step 8200 , validation loss : 0.123667\n",
      "step 8300 , validation  accuracy 0.984375\n",
      "step 8300 , validation loss : 0.0684672\n",
      "step 8400 , validation  accuracy 0.984375\n",
      "step 8400 , validation loss : 0.0853307\n",
      "step 8500 , validation  accuracy 0.953125\n",
      "step 8500 , validation loss : 0.111425\n",
      "step 8600 , validation  accuracy 0.984375\n",
      "step 8600 , validation loss : 0.0533392\n",
      "step 8700 , validation  accuracy 0.984375\n",
      "step 8700 , validation loss : 0.0290515\n",
      "step 8800 , validation  accuracy 0.984375\n",
      "step 8800 , validation loss : 0.087108\n",
      "step 8900 , validation  accuracy 0.984375\n",
      "step 8900 , validation loss : 0.0301662\n",
      "step 9000 , validation  accuracy 0.96875\n",
      "step 9000 , validation loss : 0.0594361\n",
      "step 9100 , validation  accuracy 0.984375\n",
      "step 9100 , validation loss : 0.0693341\n",
      "step 9200 , validation  accuracy 0.984375\n",
      "step 9200 , validation loss : 0.0698494\n",
      "step 9300 , validation  accuracy 0.984375\n",
      "step 9300 , validation loss : 0.0510939\n",
      "step 9400 , validation  accuracy 0.984375\n",
      "step 9400 , validation loss : 0.0777156\n",
      "step 9500 , validation  accuracy 0.984375\n",
      "step 9500 , validation loss : 0.0470739\n",
      "step 9600 , validation  accuracy 0.984375\n",
      "step 9600 , validation loss : 0.052289\n",
      "step 9700 , validation  accuracy 0.984375\n",
      "step 9700 , validation loss : 0.0276204\n",
      "step 9800 , validation  accuracy 0.984375\n",
      "step 9800 , validation loss : 0.0669181\n",
      "step 9900 , validation  accuracy 0.96875\n",
      "step 9900 , validation loss : 0.116518\n",
      "step 10000 , validation  accuracy 0.984375\n",
      "step 10000 , validation loss : 0.117183\n",
      "step 10100 , validation  accuracy 0.984375\n",
      "step 10100 , validation loss : 0.0537207\n",
      "step 10200 , validation  accuracy 1\n",
      "step 10200 , validation loss : 0.0150267\n",
      "step 10300 , validation  accuracy 0.984375\n",
      "step 10300 , validation loss : 0.0482886\n",
      "step 10400 , validation  accuracy 0.984375\n",
      "step 10400 , validation loss : 0.0506989\n",
      "step 10500 , validation  accuracy 0.96875\n",
      "step 10500 , validation loss : 0.0764445\n",
      "step 10600 , validation  accuracy 0.984375\n",
      "step 10600 , validation loss : 0.0178297\n",
      "step 10700 , validation  accuracy 0.984375\n",
      "step 10700 , validation loss : 0.0379988\n",
      "step 10800 , validation  accuracy 0.984375\n",
      "step 10800 , validation loss : 0.0401012\n",
      "step 10900 , validation  accuracy 0.984375\n",
      "step 10900 , validation loss : 0.0474056\n",
      "step 11000 , validation  accuracy 0.984375\n",
      "step 11000 , validation loss : 0.04968\n",
      "step 11100 , validation  accuracy 0.984375\n",
      "step 11100 , validation loss : 0.0652723\n",
      "step 11200 , validation  accuracy 0.984375\n",
      "step 11200 , validation loss : 0.0494585\n",
      "step 11300 , validation  accuracy 0.984375\n",
      "step 11300 , validation loss : 0.077833\n",
      "step 11400 , validation  accuracy 0.984375\n",
      "step 11400 , validation loss : 0.0441693\n",
      "step 11500 , validation  accuracy 0.984375\n",
      "step 11500 , validation loss : 0.0252492\n",
      "step 11600 , validation  accuracy 0.984375\n",
      "step 11600 , validation loss : 0.0401331\n",
      "step 11700 , validation  accuracy 0.984375\n",
      "step 11700 , validation loss : 0.0315846\n",
      "step 11800 , validation  accuracy 0.984375\n",
      "step 11800 , validation loss : 0.0286299\n",
      "step 11900 , validation  accuracy 0.984375\n",
      "step 11900 , validation loss : 0.0252477\n",
      "step 12000 , validation  accuracy 0.96875\n",
      "step 12000 , validation loss : 0.0768437\n",
      "step 12100 , validation  accuracy 0.984375\n",
      "step 12100 , validation loss : 0.026781\n",
      "step 12200 , validation  accuracy 0.984375\n",
      "step 12200 , validation loss : 0.037443\n",
      "step 12300 , validation  accuracy 0.984375\n",
      "step 12300 , validation loss : 0.070006\n",
      "step 12400 , validation  accuracy 0.984375\n",
      "step 12400 , validation loss : 0.0396945\n",
      "step 12500 , validation  accuracy 0.984375\n",
      "step 12500 , validation loss : 0.0521109\n",
      "step 12600 , validation  accuracy 0.984375\n",
      "step 12600 , validation loss : 0.051406\n",
      "step 12700 , validation  accuracy 1\n",
      "step 12700 , validation loss : 0.0125578\n",
      "step 12800 , validation  accuracy 0.984375\n",
      "step 12800 , validation loss : 0.022196\n",
      "step 12900 , validation  accuracy 0.984375\n",
      "step 12900 , validation loss : 0.0489823\n",
      "step 13000 , validation  accuracy 1\n",
      "step 13000 , validation loss : 0.0123161\n",
      "step 13100 , validation  accuracy 0.984375\n",
      "step 13100 , validation loss : 0.0362688\n",
      "step 13200 , validation  accuracy 0.984375\n",
      "step 13200 , validation loss : 0.0325734\n",
      "step 13300 , validation  accuracy 0.984375\n",
      "step 13300 , validation loss : 0.024471\n",
      "step 13400 , validation  accuracy 0.984375\n",
      "step 13400 , validation loss : 0.0150384\n",
      "step 13500 , validation  accuracy 0.984375\n",
      "step 13500 , validation loss : 0.0357728\n",
      "step 13600 , validation  accuracy 0.984375\n",
      "step 13600 , validation loss : 0.0807131\n",
      "step 13700 , validation  accuracy 0.984375\n",
      "step 13700 , validation loss : 0.0699948\n",
      "step 13800 , validation  accuracy 1\n",
      "step 13800 , validation loss : 0.0116933\n",
      "step 13900 , validation  accuracy 0.984375\n",
      "step 13900 , validation loss : 0.0347953\n",
      "step 14000 , validation  accuracy 0.984375\n",
      "step 14000 , validation loss : 0.0309063\n",
      "step 14100 , validation  accuracy 1\n",
      "step 14100 , validation loss : 0.0100189\n",
      "step 14200 , validation  accuracy 1\n",
      "step 14200 , validation loss : 0.00630473\n",
      "step 14300 , validation  accuracy 0.984375\n",
      "step 14300 , validation loss : 0.0448437\n",
      "step 14400 , validation  accuracy 0.984375\n",
      "step 14400 , validation loss : 0.0361235\n",
      "step 14500 , validation  accuracy 0.984375\n",
      "step 14500 , validation loss : 0.0191881\n",
      "step 14600 , validation  accuracy 0.984375\n",
      "step 14600 , validation loss : 0.0351496\n",
      "step 14700 , validation  accuracy 0.984375\n",
      "step 14700 , validation loss : 0.0240023\n",
      "step 14800 , validation  accuracy 1\n",
      "step 14800 , validation loss : 0.00418887\n",
      "step 14900 , validation  accuracy 1\n",
      "step 14900 , validation loss : 0.00872028\n",
      "step 15000 , validation  accuracy 1\n",
      "step 15000 , validation loss : 0.00733203\n",
      "step 15100 , validation  accuracy 0.96875\n",
      "step 15100 , validation loss : 0.0444599\n",
      "step 15200 , validation  accuracy 0.984375\n",
      "step 15200 , validation loss : 0.0224233\n",
      "step 15300 , validation  accuracy 0.984375\n",
      "step 15300 , validation loss : 0.0469011\n",
      "step 15400 , validation  accuracy 0.96875\n",
      "step 15400 , validation loss : 0.0362616\n",
      "step 15500 , validation  accuracy 1\n",
      "step 15500 , validation loss : 0.00459451\n",
      "step 15600 , validation  accuracy 0.984375\n",
      "step 15600 , validation loss : 0.030586\n",
      "step 15700 , validation  accuracy 0.984375\n",
      "step 15700 , validation loss : 0.0624333\n",
      "step 15800 , validation  accuracy 0.984375\n",
      "step 15800 , validation loss : 0.0428294\n",
      "step 15900 , validation  accuracy 0.984375\n",
      "step 15900 , validation loss : 0.0186015\n",
      "step 16000 , validation  accuracy 1\n",
      "step 16000 , validation loss : 0.0106114\n",
      "step 16100 , validation  accuracy 0.984375\n",
      "step 16100 , validation loss : 0.0595717\n",
      "step 16200 , validation  accuracy 0.984375\n",
      "step 16200 , validation loss : 0.0321007\n",
      "step 16300 , validation  accuracy 0.984375\n",
      "step 16300 , validation loss : 0.0593762\n",
      "step 16400 , validation  accuracy 0.984375\n",
      "step 16400 , validation loss : 0.0257541\n",
      "step 16500 , validation  accuracy 1\n",
      "step 16500 , validation loss : 0.0100902\n",
      "step 16600 , validation  accuracy 1\n",
      "step 16600 , validation loss : 0.0132417\n",
      "step 16700 , validation  accuracy 1\n",
      "step 16700 , validation loss : 0.0109155\n",
      "step 16800 , validation  accuracy 1\n",
      "step 16800 , validation loss : 0.0128841\n",
      "step 16900 , validation  accuracy 1\n",
      "step 16900 , validation loss : 0.0145592\n",
      "step 17000 , validation  accuracy 0.96875\n",
      "step 17000 , validation loss : 0.0624085\n",
      "step 17100 , validation  accuracy 0.96875\n",
      "step 17100 , validation loss : 0.0571576\n",
      "step 17200 , validation  accuracy 0.96875\n",
      "step 17200 , validation loss : 0.122664\n",
      "step 17300 , validation  accuracy 0.984375\n",
      "step 17300 , validation loss : 0.0713247\n",
      "step 17400 , validation  accuracy 0.984375\n",
      "step 17400 , validation loss : 0.0311149\n",
      "step 17500 , validation  accuracy 0.96875\n",
      "step 17500 , validation loss : 0.0601012\n",
      "step 17600 , validation  accuracy 0.96875\n",
      "step 17600 , validation loss : 0.0535486\n",
      "step 17700 , validation  accuracy 1\n",
      "step 17700 , validation loss : 0.00843729\n",
      "step 17800 , validation  accuracy 1\n",
      "step 17800 , validation loss : 0.00412148\n",
      "step 17900 , validation  accuracy 0.984375\n",
      "step 17900 , validation loss : 0.0485415\n",
      "step 18000 , validation  accuracy 0.984375\n",
      "step 18000 , validation loss : 0.0235165\n",
      "step 18100 , validation  accuracy 0.984375\n",
      "step 18100 , validation loss : 0.0293565\n",
      "step 18200 , validation  accuracy 1\n",
      "step 18200 , validation loss : 0.0169129\n",
      "step 18300 , validation  accuracy 1\n",
      "step 18300 , validation loss : 0.00273053\n",
      "step 18400 , validation  accuracy 1\n",
      "step 18400 , validation loss : 0.0103594\n",
      "step 18500 , validation  accuracy 0.984375\n",
      "step 18500 , validation loss : 0.0604901\n"
     ]
    }
   ],
   "source": [
    "share = int(len(val_img) / batch_size)\n",
    "for step in range(iterate):    \n",
    "    batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)\n",
    "   # batch_val_xs  , batch_val_ys = next_batch(20 , val_img , val_lab)\n",
    "    if step%100 ==0: # in here add to validation \n",
    "            for i in range(share):\n",
    "                val_accuracy ,val_loss= sess.run([accuracy ,cost], feed_dict={x:val_img[i*batch_size : (i+1)*batch_size] ,\\\n",
    "                                            y_:val_lab[i*batch_size : (i+1)*batch_size] , keep_prob: 1.0})        \n",
    "\n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            #train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n' \n",
    "            print(\"step %d , validation  accuracy %g\" %(step,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(step,val_loss))\n",
    "            #val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "\n",
    "            \n",
    "            #f.write(val_str)\n",
    "            #f.write(train_str)\n",
    "    sess.run(train_step ,feed_dict={x:batch_xs , y_:batch_ys , keep_prob : 0.7})\n",
    "print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "f.write(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "draw_graph_acc(list_train_acc,list_val_acc,100,dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_accuracy = sess.run( accuracy , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})        \n",
    "    test_loss = sess.run(cost , feed_dict = {x:test_img , y_: test_lab , keep_prob: 1.0})\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n",
    "except :\n",
    "    list_acc=[]\n",
    "    list_loss=[]\n",
    "    n_divide=len(test_img)/batch_size\n",
    "    for j in range(n_divide):\n",
    "\n",
    "        # j*batch_size :(j+1)*batch_size\n",
    "        test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "        list_acc.append(float(test_accuracy))\n",
    "        list_loss.append(float(test_loss))\n",
    "    test_accuracy , test_loss=sess.run([accuracy,cost] , feed_dict={x:test_img[(j+1)*batch_size : ] , y_:test_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "    #right above code have to modify\n",
    "\n",
    "    list_acc.append(test_accuracy)\n",
    "    list_loss.append(test_loss)\n",
    "    list_acc=np.asarray(list_acc)\n",
    "    list_loss= np.asarray(list_loss)\n",
    "\n",
    "    test_accuracy=np.mean(list_acc)\n",
    "    test_loss = np.mean(list_loss)\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
